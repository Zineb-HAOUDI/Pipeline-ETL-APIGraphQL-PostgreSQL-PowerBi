{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd39fcfb",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center; background-color:#830707ff; font-family:'Times New Roman'; \n",
    "            color: white; padding: 14px; line-height: 1.4; border-radius:20px\">\n",
    "Pipeline ETL : Conversion, stockage PostgreSQL et modélisation en étoile\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83684299",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color:#257e2aff; font-family:'Times New Roman'; \n",
    "            color: white; padding: 14px; line-height: 1.4; border-radius:20px\">\n",
    "1. Conversion du fichier JSON en CSV\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433451f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes détectées dans l'ordre : ['rowId', 'type', 'attributes.feedstock', 'attributes.manufacturer', 'buyer.nodes.name', 'buyer.nodes.parentCompany.nodes.name', 'feedstock.nodes.asset.descriptionShort', 'feedstock.nodes.rawMaterial.name', 'feedstock.nodes.quantity', 'feedstock.nodes.unit', 'transactionExpectedCertificateNomenclatures.nodes.tradeItem.descriptionShort', 'transactionExpectedCertificateNomenclatures.nodes.type', 'transactionExpectedCertificateNomenclatures.nodes.level', 'produits.nodes.asset.descriptionShort', 'produits.nodes.asset.attributes.gtin', 'produits.nodes.asset.attributes.hsCode', 'produits.nodes.asset.attributes.descriptionShort', 'produits.nodes.asset.attributes.regulatedProductName', 'produits.nodes.value', 'produits.nodes.unit', 'attributes.type', 'attributes.millsTracabilityPo', 'attributes.millsTracabilityPko', 'attributes.plantationsTracabilityPo', 'attributes.plantationsTracabilityPko', 'point_of_contact.nodes.name', 'attributes.sellingManufacturerFacilityName', 'attributes.volumeContact', 'attributes.buyerNameContact', 'attributes.manufacturerContact', 'attributes.trader', 'attributes.traderContact', 'attributes', 'produits.nodes.asset.attributes.category', 'produits.nodes.asset.attributes.isComponent', 'produits.nodes.asset.attributes.productLine', 'produits.nodes.asset.attributes.countryOfOrigin']\n",
      "✅ Export terminé : 37 colonnes exportées dans l'ordre JSON\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# === 1. Charger le fichier JSON ===\n",
    "with open(r\"result_for_query_purchase_order3.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "rows = data[\"data\"][\"biztransactions\"][\"nodes\"]\n",
    "\n",
    "# === 2. Fonction pour explorer les clés dans l'ordre ===\n",
    "def explore_value_keys(obj, prefix=\"\"):\n",
    "    keys = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            new_prefix = f\"{prefix}.{k}\" if prefix else k\n",
    "            if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "                keys.append(new_prefix)\n",
    "            else:\n",
    "                keys.extend(explore_value_keys(v, new_prefix))\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            keys.extend(explore_value_keys(item, prefix))\n",
    "    return keys\n",
    "\n",
    "# === 3. Récupérer toutes les colonnes dans l'ordre d'apparition ===\n",
    "ordered_columns = []\n",
    "for row in rows:\n",
    "    for key in explore_value_keys(row):\n",
    "        if key not in ordered_columns:\n",
    "            ordered_columns.append(key)\n",
    "\n",
    "print(\"Colonnes détectées dans l'ordre :\", ordered_columns)\n",
    "\n",
    "# === 4. Fonction générique pour extraire les valeurs ===\n",
    "def extract_nested_values(data, path):\n",
    "    results = []\n",
    "\n",
    "    def recurse(node, keys):\n",
    "        if not keys:\n",
    "            # Si valeur simple → on ajoute directement\n",
    "            if isinstance(node, (str, int, float, bool)):\n",
    "                results.append(node)   \n",
    "            return\n",
    "        key = keys[0]\n",
    "        if isinstance(node, dict) and key in node:\n",
    "            recurse(node[key], keys[1:])\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                recurse(item, keys)\n",
    "\n",
    "    recurse(data, path)\n",
    "\n",
    "    \n",
    "    # - Si plusieurs valeurs -> on garde une liste\n",
    "    # - Si une seule valeur -> on garde la valeur directement\n",
    "    if not results:\n",
    "        return None\n",
    "    elif len(results) == 1:\n",
    "        return results[0]\n",
    "    else:\n",
    "        return results   # ← garde une vraie liste Python\n",
    "\n",
    "# === 5. Construire le tableau avec les colonnes dans l'ordre ===\n",
    "records = []\n",
    "for row in rows:\n",
    "    record = {}\n",
    "    for col in ordered_columns:\n",
    "        path = col.split(\".\")\n",
    "        record[col] = extract_nested_values(row, path)\n",
    "    records.append(record)\n",
    "\n",
    "df = pd.DataFrame(records, columns=ordered_columns)\n",
    "# === 6. Explosion automatique des colonnes qui contiennent des listes ===\n",
    "for col in df.columns:\n",
    "    if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "        df = df.explode(col, ignore_index=True)\n",
    "\n",
    "\n",
    "print(f\"✅ Export terminé : {len(ordered_columns)} colonnes exportées dans l'ordre JSON\")\n",
    "\n",
    "# === 6. Export CSV\n",
    "# les colonnes avec des listes seront écrites comme \"[a, b, c]\" dans le CSV\n",
    "df.to_csv(r\"purchase_orders_full_flat37 ordered.csv\",\n",
    "          sep=\";\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7188564e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color:#257e2aff; font-family:'Times New Roman'; \n",
    "            color: white; padding: 14px; line-height: 1.4; border-radius:20px\">\n",
    "2. Anonymisation des données\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393fa59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV anonymisé écrit : C:\\Program Files\\PostgreSQL\\16\\data\\Projet Vital\\purchase_orders_full_flat37_anonymized.csv\n",
      "✅ Mapping noms écrit : C:\\Program Files\\PostgreSQL\\16\\data\\Projet Vital\\anonymization_mapping.csv\n",
      "✅ Mapping IDs écrit : C:\\Program Files\\PostgreSQL\\16\\data\\Projet Vital\\id_mapping.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import os\n",
    "\n",
    "# --- Faker en anglais  ---\n",
    "fake = Faker(\"en_US\")\n",
    "\n",
    "# --- Fichiers ---\n",
    "INPUT_CSV  = r\"purchase_orders_full_flat37 ordered.csv\"\n",
    "OUTPUT_CSV = r\"purchase_orders_full_flat37_anonymized.csv\"\n",
    "MAPPING_CSV = r\"anonymization_mapping.csv\"\n",
    "ID_MAPPING_CSV = r\"id_mapping.csv\"\n",
    "\n",
    "# --- Colonnes à anonymiser (renommées en X1, X2, ...) ---\n",
    "COLS = [\n",
    "    \"X1\",\n",
    "    \"X2\",\n",
    "    \"X3\",\n",
    "    \"X4\",\n",
    "    \"X5\",\n",
    "    \"X6\",\n",
    "]\n",
    "\n",
    "# --- Colonnes identifiants (renommées en X7, X8, ...) ---\n",
    "ID_COLS = [\n",
    "    \"X7\",\n",
    "    \"X8\",\n",
    "    \"X9\",\n",
    "]\n",
    "\n",
    "# --- Charger mapping existant si présent ---\n",
    "ANONYMIZED_VALUES = {}\n",
    "if os.path.exists(MAPPING_CSV):\n",
    "    old_map = pd.read_csv(MAPPING_CSV, dtype=str)\n",
    "    for _, row in old_map.iterrows():\n",
    "        ANONYMIZED_VALUES[row[\"original\"]] = row[\"anonymized\"]\n",
    "\n",
    "# --- Charger mapping identifiants ---\n",
    "ID_MAP = {}\n",
    "ID_COUNTER = {\"PO\": 0, \"GTIN\": 0, \"HS\": 0}\n",
    "if os.path.exists(ID_MAPPING_CSV):\n",
    "    idmap_df = pd.read_csv(ID_MAPPING_CSV, dtype=str)\n",
    "    for _, r in idmap_df.iterrows():\n",
    "        ID_MAP[r[\"original\"]] = {\"fake\": r[\"fake\"], \"type\": r[\"id_type\"]}\n",
    "        # Mise à jour compteur\n",
    "        if r[\"id_type\"] in ID_COUNTER:\n",
    "            try:\n",
    "                num = int(r[\"fake\"].split(\"_\")[-1])\n",
    "                ID_COUNTER[r[\"id_type\"]] = max(ID_COUNTER[r[\"id_type\"]], num)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# --- Fonctions ---\n",
    "def fake_value(original, col):\n",
    "    \"\"\"Anonymisation des valeurs (entreprises, contacts, etc.)\"\"\"\n",
    "    if pd.isna(original) or str(original).strip() == \"\":\n",
    "        return \"\"   # garder vide si vide à l'origine\n",
    "\n",
    "    if original not in ANONYMIZED_VALUES:\n",
    "        if \"X2\" in col or \"X6\" in col:\n",
    "            new_val = fake.company()\n",
    "        elif \"X1\" in col or \"X5\" in col:\n",
    "            new_val = fake.company() + \" Manufacturing\"\n",
    "        elif \"X4\" in col:\n",
    "            new_val = fake.name()\n",
    "        else:\n",
    "            new_val = fake.word().capitalize()\n",
    "        ANONYMIZED_VALUES[original] = new_val\n",
    "\n",
    "    return ANONYMIZED_VALUES[original]\n",
    "\n",
    "\n",
    "def _fmt(seq, width=6):\n",
    "    return str(seq).zfill(width)\n",
    "\n",
    "def anonymize_id_value(original, col):\n",
    "    \"\"\"Anonymisation des identifiants (transactions, produits, codes)\"\"\"\n",
    "    if original is None or str(original).strip() in [\"\", \"nan\", \"NaN\", \"null\", \"None\"]:\n",
    "        return \"\"\n",
    "\n",
    "    orig = str(original).strip()\n",
    "    if orig in ID_MAP:\n",
    "        return ID_MAP[orig][\"fake\"]\n",
    "\n",
    "    if col == \"X7\":\n",
    "        ID_COUNTER[\"PO\"] += 1\n",
    "        fakev = f\"PO_{_fmt(ID_COUNTER['PO'])}\"\n",
    "        id_type = \"PO\"\n",
    "    elif \"X8\" in col:\n",
    "        ID_COUNTER[\"GTIN\"] += 1\n",
    "        fakev = f\"FAKE_PRODUIT_{_fmt(ID_COUNTER['GTIN'])}\"\n",
    "        id_type = \"GTIN\"\n",
    "    elif \"X9\" in col:\n",
    "        ID_COUNTER[\"HS\"] += 1\n",
    "        fakev = f\"FAKE_CODE_{_fmt(ID_COUNTER['HS'])}\"\n",
    "        id_type = \"HS\"\n",
    "    else:\n",
    "        fakev = f\"FAKE_{hash(orig) % 1000000}\"\n",
    "        id_type = \"OTHER\"\n",
    "\n",
    "    ID_MAP[orig] = {\"fake\": fakev, \"type\": id_type}\n",
    "    return fakev\n",
    "\n",
    "# --- Traitement par chunks ---\n",
    "chunksize = 100_000\n",
    "first = True\n",
    "\n",
    "for chunk in pd.read_csv(INPUT_CSV, sep=\";\", dtype=str, chunksize=chunksize, low_memory=False, encoding=\"utf-8\"):\n",
    "    # anonymiser noms\n",
    "    for col in COLS:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].apply(lambda x: fake_value(x, col))\n",
    "\n",
    "    # anonymiser identifiants\n",
    "    for col in ID_COLS:\n",
    "        if col in chunk.columns:\n",
    "            chunk[col] = chunk[col].apply(lambda x: anonymize_id_value(x, col))\n",
    "\n",
    "    # écrire fichier\n",
    "    chunk.to_csv(OUTPUT_CSV, sep=\";\", index=False, encoding=\"utf-8\", mode=\"w\" if first else \"a\", header=first)\n",
    "    first = False\n",
    "\n",
    "# --- Sauvegarde mapping noms ---\n",
    "map_df = pd.DataFrame([{\"original\": k, \"anonymized\": v} for k, v in ANONYMIZED_VALUES.items()])\n",
    "map_df.to_csv(MAPPING_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# --- Sauvegarde mapping identifiants ---\n",
    "if ID_MAP:\n",
    "    rows = [{\"original\": k, \"fake\": v[\"fake\"], \"id_type\": v[\"type\"]} for k, v in ID_MAP.items()]\n",
    "    idmap_df = pd.DataFrame(rows)\n",
    "    idmap_df.to_csv(ID_MAPPING_CSV, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ CSV anonymisé écrit : {OUTPUT_CSV}\")\n",
    "print(f\"✅ Mapping noms écrit : {MAPPING_CSV}\")\n",
    "print(f\"✅ Mapping IDs écrit : {ID_MAPPING_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc9da2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color:#257e2aff; font-family:'Times New Roman'; \n",
    "            color: white; padding: 14px; line-height: 1.4; border-radius:20px\">\n",
    "3. Échantillonnage des données\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf058f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Échantillon de 10000 lignes créé : C:\\Program Files\\PostgreSQL\\16\\data\\Projet Vital\\purchase_orders_sample_random.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "INPUT_CSV = r\"purchase_orders_full_flat37_anonymized.csv\"\n",
    "OUTPUT_CSV = r\"purchase_orders_sample_random.csv\"\n",
    "\n",
    "# Nombre de lignes à garder\n",
    "N = 100000   # tu peux mettre 5000 ou 20000 si tu veux\n",
    "\n",
    "# Charger seulement N lignes\n",
    "df = pd.read_csv(INPUT_CSV, sep=\";\", nrows=N)\n",
    "\n",
    "# Sauvegarder l'échantillon\n",
    "df.to_csv(OUTPUT_CSV, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Échantillon de {N} lignes créé : {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232de7b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color:#257e2aff; font-family:'Times New Roman';\n",
    "            color: white; padding: 14px; line-height: 1.4; border-radius:20px;\">\n",
    "4. Création de la base Projet sur PostgreSQL et chargement des données\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Base 'Projet' supprimée et recréée avec succès\n",
      "✅ Données CSV importées dans PostgreSQL (staging_orders)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Charger les variables depuis le fichier .env\n",
    "load_dotenv()\n",
    "\n",
    "# === 1. Charger le fichier CSV ===\n",
    "csv_file = r\"purchase_orders_sample_random.csv\"\n",
    "df = pd.read_csv(csv_file, sep=\";\")\n",
    "\n",
    "# Colonnes numériques (renommées en X1, X2, ...)\n",
    "numeric_cols = [\n",
    "    \"X1\",\n",
    "    \"X2\",\n",
    "    \"X3\",\n",
    "    \"X4\",\n",
    "    \"X5\",\n",
    "    \"X6\",\n",
    "    \"X7\"\n",
    "]\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# === 2. Supprimer et recréer la base Projet ===\n",
    "# AUTOCOMMIT pour DROP/CREATE DATABASE\n",
    "password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "\n",
    "admin_engine = create_engine(\n",
    "    f\"postgresql+psycopg2://postgres:{password}@localhost:5432/postgres\",\n",
    "    isolation_level=\"AUTOCOMMIT\"\n",
    ")\n",
    "\n",
    "with admin_engine.begin() as conn:\n",
    "    # Fermer toutes les connexions actives sur Projet\n",
    "    conn.execute(text(\"\"\"\n",
    "        SELECT pg_terminate_backend(pid)\n",
    "        FROM pg_stat_activity\n",
    "        WHERE datname = 'Projet' AND pid <> pg_backend_pid();\n",
    "    \"\"\"))\n",
    "    \n",
    "    # Supprimer puis recréer la base\n",
    "    conn.execute(text('DROP DATABASE IF EXISTS \"Projet\";'))\n",
    "    conn.execute(text(\"CREATE DATABASE \\\"Projet\\\" WITH ENCODING 'UTF8' TEMPLATE template1;\"))\n",
    "\n",
    "print(\"🗑️ Base 'Projet' supprimée et recréée avec succès\")\n",
    "\n",
    "# === 3. Connexion à la nouvelle base Projet ===\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://postgres:{password}@localhost:5432/Projet\"\n",
    ")\n",
    "\n",
    "# === 4. Charger le CSV dans une table de staging ===\n",
    "df.to_sql(\"staging_orders\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(\"✅ Données CSV importées dans PostgreSQL (staging_orders)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be96b0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color:#257e2aff; font-family:'Times New Roman';\n",
    "            color: white; padding: 14px; line-height: 1.4; border-radius:20px;\">\n",
    "5. Création des tables pour la modélisation en étoile\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4589e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tables de dimensions et de faits créées dans PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "# === 4. Créer les tables de dimensions et de faits (version anonymisée) ===\n",
    "schema_sql = \"\"\"\n",
    "\n",
    "DROP TABLE IF EXISTS Fact_Orders CASCADE;\n",
    "DROP SEQUENCE IF EXISTS fact_orders_id_fact_seq CASCADE;\n",
    "DROP TABLE IF EXISTS Dim_X1 CASCADE;\n",
    "DROP TABLE IF EXISTS Dim_X2 CASCADE;\n",
    "DROP TABLE IF EXISTS Dim_X3 CASCADE;\n",
    "DROP TABLE IF EXISTS Dim_X4 CASCADE;\n",
    "DROP TABLE IF EXISTS Dim_X5 CASCADE;\n",
    "DROP TABLE IF EXISTS Dim_X6 CASCADE;\n",
    "DROP TABLE IF EXISTS Dim_X7 CASCADE;\n",
    "\n",
    "-- Table X1\n",
    "CREATE TABLE IF NOT EXISTS Dim_X1 (\n",
    "    id_x1 SERIAL PRIMARY KEY,\n",
    "    \"X1\" TEXT,\n",
    "    \"X2\" TEXT,\n",
    "    \"X3\" TEXT\n",
    ");\n",
    "\n",
    "-- Table X2\n",
    "CREATE TABLE IF NOT EXISTS Dim_X2 (\n",
    "    id_x2 SERIAL PRIMARY KEY,\n",
    "    \"X4\" TEXT,\n",
    "    \"X5\" TEXT,\n",
    "    \"X6\" TEXT,\n",
    "    \"X7\" TEXT,\n",
    "    \"X8\" TEXT,\n",
    "    \"X9\" TEXT,\n",
    "    \"X10\" TEXT,\n",
    "    \"X11\" TEXT,\n",
    "    \"X12\" TEXT,\n",
    "    \"X13\" TEXT\n",
    ");\n",
    "\n",
    "-- Table X3\n",
    "CREATE TABLE IF NOT EXISTS Dim_X3 (\n",
    "    id_x3 SERIAL PRIMARY KEY,\n",
    "    \"X14\" TEXT,\n",
    "    \"X15\" TEXT,\n",
    "    \"X16\" TEXT,\n",
    "    \"X17\" TEXT\n",
    ");\n",
    "\n",
    "-- Table X4\n",
    "CREATE TABLE IF NOT EXISTS Dim_X4 (\n",
    "    id_x4 SERIAL PRIMARY KEY,\n",
    "    \"X18\" TEXT,\n",
    "    \"X19\" TEXT,\n",
    "    \"X20\" TEXT\n",
    ");\n",
    "\n",
    "-- Table X5\n",
    "CREATE TABLE IF NOT EXISTS Dim_X5 (\n",
    "    id_x5 SERIAL PRIMARY KEY,\n",
    "    \"X21\" TEXT,\n",
    "    \"X22\" TEXT\n",
    ");\n",
    "\n",
    "-- Table X6\n",
    "CREATE TABLE IF NOT EXISTS Dim_X6 (\n",
    "    id_x6 SERIAL PRIMARY KEY,\n",
    "    \"X23\" TEXT\n",
    ");\n",
    "\n",
    "-- Table X7\n",
    "CREATE TABLE IF NOT EXISTS Dim_X7 (\n",
    "    id_x7 SERIAL PRIMARY KEY,\n",
    "    \"X24\" TEXT,\n",
    "    \"X25\" TEXT,\n",
    "    \"X26\" TEXT\n",
    ");\n",
    "\n",
    "-- Table des faits\n",
    "CREATE TABLE IF NOT EXISTS Fact_Orders (\n",
    "    id_fact SERIAL PRIMARY KEY,\n",
    "    \"X27\" TEXT,\n",
    "    \"X28\" TEXT,\n",
    "    id_x1 INT REFERENCES Dim_X1(id_x1),\n",
    "    id_x2 INT REFERENCES Dim_X2(id_x2),\n",
    "    id_x3 INT REFERENCES Dim_X3(id_x3),\n",
    "    id_x4 INT REFERENCES Dim_X4(id_x4),\n",
    "    id_x5 INT REFERENCES Dim_X5(id_x5),\n",
    "    id_x6 INT REFERENCES Dim_X6(id_x6),\n",
    "    id_x7 INT REFERENCES Dim_X7(id_x7),\n",
    "    \"X29\" NUMERIC,\n",
    "    \"X30\" NUMERIC,\n",
    "    \"X31\" NUMERIC,\n",
    "    \"X32\" NUMERIC,\n",
    "    \"X33\" NUMERIC,\n",
    "    \"X34\" NUMERIC,\n",
    "    \"X35\" NUMERIC\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(schema_sql))\n",
    "    conn.commit()\n",
    "\n",
    "print(\"✅ Tables de dimensions et de faits créées dans PostgreSQL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fa30f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color:#257e2aff; font-family:'Times New Roman';\n",
    "            color: white; padding: 14px; line-height: 1.4; border-radius:20px;\">\n",
    "5. Alimentation des tables avec le fichier de base\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84795453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dimensions alimentées avec succès\n",
      "✅ Table des faits alimentée avec succès\n"
     ]
    }
   ],
   "source": [
    "## === 5. Alimentation des tables de dimensions (version anonymisée) ===\n",
    "with engine.begin() as conn:\n",
    "    # Dim_X1\n",
    "    conn.execute(text(\"\"\"\n",
    "        INSERT INTO Dim_X1 (\"X1\", \"X2\", \"X3\")\n",
    "        SELECT DISTINCT \"X1\", \"X2\", \"X3\"\n",
    "        FROM staging_orders\n",
    "        WHERE \"X1\" IS NOT NULL;\n",
    "    \"\"\"))\n",
    "\n",
    "    # Dim_X2\n",
    "    conn.execute(text(\"\"\"\n",
    "        INSERT INTO Dim_X2 (\"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\", \"X13\")\n",
    "        SELECT DISTINCT \"X4\", \"X5\", \"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\", \"X13\"\n",
    "        FROM staging_orders;\n",
    "    \"\"\"))\n",
    "\n",
    "    # Dim_X3\n",
    "    conn.execute(text(\"\"\"\n",
    "        INSERT INTO Dim_X3 (\"X14\", \"X15\", \"X16\", \"X17\")\n",
    "        SELECT DISTINCT \"X14\", \"X15\", \"X16\", \"X17\"\n",
    "        FROM staging_orders;\n",
    "    \"\"\"))\n",
    "\n",
    "    # Dim_X4\n",
    "    conn.execute(text(\"\"\"\n",
    "        INSERT INTO Dim_X4 (\"X18\", \"X19\", \"X20\")\n",
    "        SELECT DISTINCT \"X18\", \"X19\", \"X20\"\n",
    "        FROM staging_orders;\n",
    "    \"\"\"))\n",
    "\n",
    "    # Dim_X5\n",
    "    conn.execute(text(\"\"\"\n",
    "        INSERT INTO Dim_X5 (\"X21\", \"X22\")\n",
    "        SELECT DISTINCT \"X21\", \"X22\"\n",
    "        FROM staging_orders;\n",
    "    \"\"\"))\n",
    "\n",
    "    # Dim_X6\n",
    "    conn.execute(text(\"\"\"\n",
    "        INSERT INTO Dim_X6 (\"X23\")\n",
    "        SELECT DISTINCT \"X23\"\n",
    "        FROM staging_orders;\n",
    "    \"\"\"))\n",
    "\n",
    "    # Dim_X7\n",
    "    conn.execute(text(\"\"\"\n",
    "        INSERT INTO Dim_X7 (\"X24\", \"X25\", \"X26\")\n",
    "        SELECT DISTINCT \"X24\", \"X25\", \"X26\"\n",
    "        FROM staging_orders;\n",
    "    \"\"\"))\n",
    "\n",
    "print(\"✅ Dimensions alimentées avec succès\")\n",
    "\n",
    "\n",
    "# === 6. Alimentation de la table des faits (version anonymisée) ===\n",
    "with engine.begin() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        INSERT INTO Fact_Orders (\n",
    "            \"X27\", \"X28\",\n",
    "            id_x1, id_x2, id_x3, id_x4, id_x5, id_x6, id_x7,\n",
    "            \"X29\", \"X30\",\n",
    "            \"X31\", \"X32\",\n",
    "            \"X33\", \"X34\",\n",
    "            \"X35\"\n",
    "        )\n",
    "        SELECT\n",
    "            staging_orders.\"X27\",\n",
    "            staging_orders.\"X28\",\n",
    "            Dim_X1.id_x1,\n",
    "            Dim_X2.id_x2,\n",
    "            Dim_X3.id_x3,\n",
    "            Dim_X4.id_x4,\n",
    "            Dim_X5.id_x5,\n",
    "            Dim_X6.id_x6,\n",
    "            Dim_X7.id_x7,\n",
    "            staging_orders.\"X29\",\n",
    "            staging_orders.\"X30\",\n",
    "            staging_orders.\"X31\",\n",
    "            staging_orders.\"X32\",\n",
    "            staging_orders.\"X33\",\n",
    "            staging_orders.\"X34\",\n",
    "            staging_orders.\"X35\"\n",
    "        FROM staging_orders\n",
    "        LEFT JOIN Dim_X1 ON staging_orders.\"X1\" = Dim_X1.\"X1\"\n",
    "        LEFT JOIN Dim_X2 ON staging_orders.\"X4\" = Dim_X2.\"X4\"\n",
    "        LEFT JOIN Dim_X3 ON staging_orders.\"X14\" = Dim_X3.\"X14\"\n",
    "        LEFT JOIN Dim_X4 ON staging_orders.\"X18\" = Dim_X4.\"X18\"\n",
    "        LEFT JOIN Dim_X5 ON staging_orders.\"X21\" = Dim_X5.\"X21\"\n",
    "        LEFT JOIN Dim_X6 ON staging_orders.\"X23\" = Dim_X6.\"X23\"\n",
    "        LEFT JOIN Dim_X7 ON staging_orders.\"X24\" = Dim_X7.\"X24\";\n",
    "    \"\"\"))\n",
    "\n",
    "print(\"✅ Table des faits alimentée avec succès\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
